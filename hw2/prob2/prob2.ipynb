{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sn\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pygal\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as trns\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "visual_folder = \"./visual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class animateDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.files = os.listdir(os.getcwd()+'/data')\n",
    "        self.transform = trns.Compose([trns.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        image = Image.open(os.getcwd()+'/data/'+self.files[index]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset = animateDataset()\n",
    "train_sz = int(len(dataset)*0.7)\n",
    "test_sz = len(dataset)-train_sz\n",
    "train_set,test_set = random_split(dataset,[train_sz,test_sz])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEmodule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAEmodule, self).__init__()\n",
    "        self.image_size = 64\n",
    "        self.encoder_channel = 64\n",
    "        self.decoder_channel = 64\n",
    "        self.latent_size = 512\n",
    "        # ----------------------------------------------------------------------------------------                          \n",
    "        self.en_conv1 = self.conv_layer(3,self.encoder_channel)\n",
    "        self.en_conv2 = self.conv_layer(self.encoder_channel,self.encoder_channel*2)\n",
    "        self.en_conv3 = self.conv_layer(self.encoder_channel*2,self.encoder_channel*4)\n",
    "        self.en_conv4 = self.conv_layer(self.encoder_channel*4,self.encoder_channel*8)\n",
    "        self.en_fc1 = nn.Sequential(nn.Linear(self.encoder_channel*8*(self.image_size//16)**2, self.latent_size),\n",
    "                                    nn.BatchNorm1d(self.latent_size))\n",
    "        self.en_fc2 = nn.Sequential(nn.Linear(self.encoder_channel*8*(self.image_size//16)**2, self.latent_size),\n",
    "                                    nn.BatchNorm1d(self.latent_size))\n",
    "        # ----------------------------------------------------------------------------------------                          \n",
    "        self.de_fc1 = nn.Linear(self.latent_size, self.decoder_channel*4*(self.image_size//8)**2)\n",
    "        self.de_fc2 = nn.Sequential(nn.BatchNorm2d(self.decoder_channel*4),\n",
    "                                    nn.ReLU())\n",
    "        self.de_conv1 = self.de_conv_layer(self.decoder_channel*4, self.decoder_channel*4)\n",
    "        self.de_conv2 = self.de_conv_layer(self.decoder_channel*4, self.decoder_channel*2)\n",
    "        self.de_conv3 = self.de_conv_layer(self.decoder_channel*2, self.decoder_channel//2)\n",
    "        self.dc1 = nn.ConvTranspose2d(self.decoder_channel//2, 3, 5, padding=2)\n",
    "        \n",
    "    \n",
    "    def conv_layer(self,input_channel, output_channel, kernel=5, stride=2, padding=2):\n",
    "        return nn.Sequential(nn.Conv2d(input_channel, output_channel, kernel, stride=stride, padding=padding),\n",
    "                             nn.BatchNorm2d(output_channel),\n",
    "                             nn.ReLU())\n",
    "\n",
    "    def de_conv_layer(self,input_channel, output_channel, kernel=6, stride=2, padding=2):\n",
    "        return nn.Sequential(nn.ConvTranspose2d(input_channel, output_channel, kernel, stride=stride, padding=padding),\n",
    "                         nn.BatchNorm2d(output_channel),\n",
    "                         nn.ReLU())\n",
    "    \n",
    "    def encoder(self,x):\n",
    "        x = self.en_conv1(x)\n",
    "        x = self.en_conv2(x)\n",
    "        x = self.en_conv3(x)\n",
    "        x = self.en_conv4(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        mean = self.en_fc1(x)\n",
    "        logvar = self.en_fc2(x)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameter(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps*std\n",
    "    \n",
    "    def decoder(self,x):\n",
    "        # forward pass\n",
    "        x = self.de_fc1(x)\n",
    "        x = x.view(-1, self.decoder_channel*4, (self.image_size//8), (self.image_size//8))\n",
    "        x = self.de_fc2(x)\n",
    "        x = self.de_conv1(x)\n",
    "        x = self.de_conv2(x)\n",
    "        x = self.de_conv3(x)\n",
    "        x = self.dc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        x = self.reparameter(mean, logvar)\n",
    "        x = self.decoder(x)\n",
    "        return x ,mean ,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, logvar):\n",
    "    BCE = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "\n",
    "#     KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    KLD = 0\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss:5802.5401:  13%|█▎        | 32/240 [11:14<1:13:25, 21.18s/it]"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epoch = 240\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vae = VAEmodule()\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "iteration = 0\n",
    "ELBO = []\n",
    "ELBO_logs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "t = trange(epoch)\n",
    "\n",
    "for e in t:  # loop over the dataset multiple times    \n",
    "    for imgs in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat ,mean ,logvar = vae(imgs)\n",
    "        elbo = loss_function(imgs, x_hat ,mean ,logvar)\n",
    "        elbo.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ELBO.append(elbo.item()/imgs.size(0))\n",
    "        \n",
    "    ELBO_logs.append(np.mean(ELBO))\n",
    "    ELBO = []\n",
    "\n",
    "    t.set_description(\"training loss:%.4f\"%(ELBO_logs[-1]))\n",
    "\n",
    "vae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Evidence Lower Bound')\n",
    "plt.xlabel('epochs')\n",
    "plt.plot(ELBO_logs)\n",
    "plt.savefig(visual_folder+'/train_elbo.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(imgs, file_name):\n",
    "    torchvision.utils.save_image(imgs,visual_folder+'/'+file_name)\n",
    "    imgs = torchvision.utils.make_grid(imgs)\n",
    "    toPIL = trns.ToPILImage()\n",
    "    imgs = toPIL(imgs)\n",
    "    plt.imshow(imgs)\n",
    "    plt.show()\n",
    "    \n",
    "def reconstruct(dataloader, model, batch_num=1):\n",
    "    i = 0\n",
    "    origin = None\n",
    "    recons = None\n",
    "    for imgs in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        x_hat ,mean ,logvar = model(imgs)\n",
    "        if recons is None : \n",
    "            origin=imgs\n",
    "            recons=x_hat\n",
    "        else: \n",
    "            recons=torch.cat((x_hat,recons),0)\n",
    "            origin=torch.cat((imgs,origin),0)\n",
    "        \n",
    "        i+=1\n",
    "        if i==batch_num:\n",
    "            return recons.cpu() ,origin.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, origins = reconstruct(test_loader, vae)\n",
    "save_imgs(imgs,'reconstruct.png')\n",
    "save_imgs(origins,'origin.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = trns.ToPILImage()\n",
    "sample = torch.randn(batch_size, vae.latent_size).to(device)\n",
    "imgs = vae.decoder(sample).cpu()\n",
    "save_imgs(imgs,'test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(t,size):\n",
    "    interval = (t[1]-t[0])/size\n",
    "    new = t[0]\n",
    "    for i in range(1,size):\n",
    "        new = torch.cat((new,t[0]+interval*i),0)\n",
    "    new = new.view(size,-1)\n",
    "    print(new.shape)\n",
    "    return new\n",
    "\n",
    "sample = torch.randn(2, vae.latent_size).to(device)\n",
    "imgs = vae.decoder(sample).cpu()\n",
    "save_imgs(imgs,'interpolate.png')\n",
    "\n",
    "sample = interpolate(sample,16)\n",
    "\n",
    "imgs = vae.decoder(sample).cpu()\n",
    "save_imgs(imgs,'interpolate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
